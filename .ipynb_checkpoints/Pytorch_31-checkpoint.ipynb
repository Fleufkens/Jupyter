{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "0.6.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t,dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()\n",
    "                                                                                         ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 47561 loss: 331.19654743373394\n",
      "epoch 1 total_correct: 51637 loss: 224.3363164961338\n",
      "epoch 2 total_correct: 52241 loss: 210.0451462417841\n",
      "epoch 3 total_correct: 52515 loss: 201.61196699738503\n",
      "epoch 4 total_correct: 52849 loss: 194.60017451643944\n",
      "epoch 5 total_correct: 53030 loss: 188.1753448098898\n",
      "epoch 6 total_correct: 53079 loss: 186.44271056354046\n",
      "epoch 7 total_correct: 53351 loss: 182.42857539653778\n",
      "epoch 8 total_correct: 53324 loss: 180.34868792444468\n",
      "epoch 9 total_correct: 53436 loss: 178.21647591143847\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100,shuffle=True)\n",
    "optimiser = optim.Adam(network.parameters(),lr=0.01)\n",
    "\n",
    "images,labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images',grid)\n",
    "tb.add_graph(network,images)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        total_correct+=get_num_correct(preds,labels)\n",
    "        \n",
    "\n",
    "    tb.add_scalar('Loss',total_loss,epoch)\n",
    "    tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "    tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
    "\n",
    "    for name, weight in network.named_parameters():\n",
    "        tb.add_histogram(name,weight,epoch)\n",
    "        tb.add_histogram(f'{name}.grad',weight.grad,epoch)\n",
    "        \n",
    "\n",
    "    print(\"epoch\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss )\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([12, 6, 5, 5])\n",
      "conv2.bias torch.Size([12])\n",
      "fc1.weight torch.Size([120, 192])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([60, 120])\n",
      "fc2.bias torch.Size([60])\n",
      "out.weight torch.Size([10, 60])\n",
      "out.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    " for name, weight in network.named_parameters():\n",
    "    print(name,weight.shape)     #grad kan niet direct achter name, omdat name hier de string is, niet meer de parameter. bij weight nog wel, dus gaat daar goed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight.grad torch.Size([6, 1, 5, 5])\n",
      "conv1.bias.grad torch.Size([6])\n",
      "conv2.weight.grad torch.Size([12, 6, 5, 5])\n",
      "conv2.bias.grad torch.Size([12])\n",
      "fc1.weight.grad torch.Size([120, 192])\n",
      "fc1.bias.grad torch.Size([120])\n",
      "fc2.weight.grad torch.Size([60, 120])\n",
      "fc2.bias.grad torch.Size([60])\n",
      "out.weight.grad torch.Size([10, 60])\n",
      "out.bias.grad torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, weight in network.named_parameters():\n",
    "    print(f'{name}.grad',weight.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 100 lr 0.01 epoch 0 total_correct: 47913 loss: 32573.806416988373\n",
      "batch size 100 lr 0.01 epoch 1 total_correct: 51663 loss: 22555.30280917883\n",
      "batch size 100 lr 0.001 epoch 0 total_correct: 40952 loss: 50420.70196568966\n",
      "batch size 100 lr 0.001 epoch 1 total_correct: 47645 loss: 32774.021354317665\n",
      "batch size 100 lr 0.0001 epoch 0 total_correct: 29646 loss: 82796.26566767693\n",
      "batch size 100 lr 0.0001 epoch 1 total_correct: 42359 loss: 46919.58585977554\n",
      "batch size 100 lr 1e-05 epoch 0 total_correct: 8206 loss: 137085.5415582657\n",
      "batch size 100 lr 1e-05 epoch 1 total_correct: 23888 loss: 126676.52665376663\n",
      "batch size 1000 lr 0.01 epoch 0 total_correct: 37420 loss: 60843.49250793457\n",
      "batch size 1000 lr 0.01 epoch 1 total_correct: 47575 loss: 32757.487058639526\n",
      "batch size 1000 lr 0.001 epoch 0 total_correct: 27512 loss: 94688.59058618546\n",
      "batch size 1000 lr 0.001 epoch 1 total_correct: 42904 loss: 43685.304403305054\n",
      "batch size 1000 lr 0.0001 epoch 0 total_correct: 6000 loss: 137812.90698051453\n",
      "batch size 1000 lr 0.0001 epoch 1 total_correct: 13334 loss: 134316.0319328308\n",
      "batch size 1000 lr 1e-05 epoch 0 total_correct: 6000 loss: 138348.85573387146\n",
      "batch size 1000 lr 1e-05 epoch 1 total_correct: 6000 loss: 138277.4794101715\n",
      "batch size 10000 lr 0.01 epoch 0 total_correct: 13891 loss: 127828.13549041748\n",
      "batch size 10000 lr 0.01 epoch 1 total_correct: 27063 loss: 80190.42491912842\n",
      "batch size 10000 lr 0.001 epoch 0 total_correct: 6000 loss: 137718.2149887085\n",
      "batch size 10000 lr 0.001 epoch 1 total_correct: 12878 loss: 134973.37102890015\n",
      "batch size 10000 lr 0.0001 epoch 0 total_correct: 6000 loss: 138230.8053970337\n",
      "batch size 10000 lr 0.0001 epoch 1 total_correct: 6000 loss: 138149.6238708496\n",
      "batch size 10000 lr 1e-05 epoch 0 total_correct: 7117 loss: 138299.36981201172\n",
      "batch size 10000 lr 1e-05 epoch 1 total_correct: 7270 loss: 138287.73498535156\n"
     ]
    }
   ],
   "source": [
    "#we resetten gewoon vrolijk\n",
    "batch_size_list = [100, 1000, 10000] #we zien dat hoe groter batch size, hoe slechter de guess\n",
    "lr_list = [0.01, .001, .0001, .00001] #we zien dat ook kleinste lr 't beste werkt. we skippen nog niet 'genoeg iig' de optima\n",
    "\n",
    "for batch_size in batch_size_list:#dit werkt dus zonder te initieren, hij begrijpt zelf wat ie moet zijn. zelfde voor lr\n",
    "    for lr in lr_list:\n",
    "        network = Network() #reset dus elke keer ook de weights op deze manier denk ik\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "        optimiser = optim.Adam(network.parameters(),lr=lr)\n",
    "\n",
    "        images,labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment = f' batch_size={batch_size} lr ={lr}'#geloof dat je dit alles kunt noemen, parameters zelfde naam geven is super stupide, les 1\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        tb.add_image('images',grid)\n",
    "        tb.add_graph(network,images)\n",
    "        \n",
    "        for epoch in range(2):\n",
    "    \n",
    "            total_loss=0\n",
    "            total_loss_b=0\n",
    "            total_correct=0\n",
    "    \n",
    "            for batch in train_loader:\n",
    "                images, labels = batch\n",
    "        \n",
    "                preds = network(images)\n",
    "                loss = F.cross_entropy(preds,labels)\n",
    "            \n",
    "                optimiser.zero_grad()\n",
    "                loss.backward() #zoals je ziet wordt er al in epoch 1 geoptimised. dus de initial guess wordt wel zeker aangetast door de learning rate\n",
    "                optimiser.step()\n",
    "        \n",
    "                total_loss+=loss.item()*batch_size #loss is average. to be able to compare the total if we worked with different batch sizes in different, we have to make average -> total\n",
    "                total_correct+=get_num_correct(preds,labels)\n",
    "        \n",
    "\n",
    "            tb.add_scalar('Loss',total_loss,epoch)\n",
    "            tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "            tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
    "\n",
    "            for name, weight in network.named_parameters():\n",
    "                tb.add_histogram(name,weight,epoch)\n",
    "                tb.add_histogram(f'{name}.grad',weight.grad,epoch)\n",
    "        \n",
    "\n",
    "            print(\"batch size\",batch_size,\"lr\",lr,\"epoch\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss )\n",
    "    \n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    lr= [0.01, 0.001]\n",
    "    ,batch_size = [10, 100, 1000]\n",
    "    ,shuffel = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.01, 0.001], [10, 100, 1000], [True, False]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_values = [v for v in parameters.values()]   #return value list v for each v in the parameter dictionary. basically sort of creating a tensor where concatenating different v's after one another? \n",
    "param_values #so lr/batch_size/shuffle are al v's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': [0.01, 0.001], 'batch_size': [10, 100, 1000], 'shuffel': [True, False]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'v' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-785552c78d3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mparam\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'v' is not defined"
     ]
    }
   ],
   "source": [
    "#param = [v in parameters.values()] #dit werkt dus niet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 10 True\n",
      "0.01 10 False\n",
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.001 10 True\n",
      "0.001 10 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n"
     ]
    }
   ],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values): #star here tells product to treat each value in the list as argument, instead of treating the list itself as the argument\n",
    "    print(lr,batch_size,shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.001]\n",
      "[10, 100, 1000]\n",
      "[True, False]\n"
     ]
    }
   ],
   "source": [
    "for lr in (param_values):\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values): #star here tells product to treat each value in the list as argument, instead of treating the list itself as the argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
