{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run</th>\n",
       "      <th>epoch</th>\n",
       "      <th>loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>epoch duration</th>\n",
       "      <th>run duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.297156</td>\n",
       "      <td>0.1611</td>\n",
       "      <td>8.554125</td>\n",
       "      <td>13.411089</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   run  epoch      loss  accuracy  epoch duration  run duration     lr  \\\n",
       "0    1      1  2.297156    0.1611        8.554125     13.411089  0.001   \n",
       "\n",
       "   batch_size  \n",
       "0       10000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "from collections import OrderedDict\n",
    "\n",
    "import os \n",
    "\n",
    "#NameFile\n",
    "class NameFile():\n",
    "    @staticmethod#static? iets met dat je m kan callen using the class itself. don't need an instance of the class, to call the method. i guess dat je m dus niet eerst hoeft te initieren alszijnde type x? \n",
    "    def nameit(params): #ga er dus vanuit dat je alleen keys and values hebt, dus alleen 0 en 1 voor die ene index. wellicht gaat dit dus ooit mis, makkelijk te herstellen\n",
    "\n",
    "        items_hier = list(params.items()) #keys+values\n",
    "        num_k=0 #amount of keys present\n",
    "        comment = '' #the string for the filename\n",
    "        \n",
    "        #make the comment by looping over keys and values\n",
    "        for k in params.keys():\n",
    "            comment += f'{items_hier[num_k][0]}=' #add the key\n",
    "            for v in items_hier[num_k][1]:\n",
    "                comment +=(f'{v}_') #add the values\n",
    "            num_k+=1 #for indexing next loop to get next keys/values in list\n",
    "        comment+='results'\n",
    "        return comment\n",
    "\n",
    "#RunBuilder\n",
    "class RunBuilder():\n",
    "    @staticmethod#static? iets met dat je m kan callen using the class itself. don't need an instance of the class, to call the method. i guess dat je m dus niet eerst hoeft te initieren alszijnde type x? \n",
    "    def get_runs(params):\n",
    "        \n",
    "        Run = namedtuple('Run',params.keys())#die ordereddicttionary heeft keys and values. dit heb je wel eens eerder gezien I guess.\n",
    "                                             #blijkbaar maakt ie een mooie string als je die keys zo oproept. test dat even.\n",
    "        \n",
    "        runs = []\n",
    "        for v in product(*params.values()): #dit doet dus iets dat ie per value combo nieuwe iteratie doet\n",
    "            runs.append(Run(*v))\n",
    "            \n",
    "        return runs\n",
    "\n",
    "#RunManager\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.epoch_count = 0\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        self.epoch_start_time = None\n",
    "        \n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        \n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "\n",
    "    def begin_run(self,run,network,loader): #die self is dus gewoon de variabel naam links van de streep\n",
    "        #start time for a run, parameters run added, run_count+1 (stays same for all epochs)\n",
    "        #network copied, loader copied, name given in tb. \n",
    "        self.run_start_time = time.time()\n",
    "        \n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment=f'-{run}')\n",
    "        \n",
    "        images,labels = next(iter(self.loader)) #misschien wel gewoon plaatjes inladen voor foto'tje in tensorboard\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "        \n",
    "        self.tb.add_image('images',grid)\n",
    "        self.tb.add_graph(self.network,images)\n",
    "        \n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.epoch_count = 0\n",
    "\n",
    "    def begin_epoch(self):\n",
    "        self.epoch_start_time = time.time()\n",
    "        \n",
    "        self.epoch_count += 1\n",
    "        self.epoch_loss = 0\n",
    "        self.epoch_num_correct = 0\n",
    "        \n",
    "    def end_epoch(self):\n",
    "        \n",
    "        epoch_duration = time.time()-self.epoch_start_time\n",
    "        run_duration = time.time()-self.run_start_time\n",
    "        \n",
    "        loss = self.epoch_loss/len(self.loader.dataset)\n",
    "        accuracy = self.epoch_num_correct/len(self.loader.dataset)\n",
    "        \n",
    "        self.tb.add_scalar('Loss',loss,self.epoch_count)\n",
    "        self.tb.add_scalar('Accuracy',accuracy,self.epoch_count)\n",
    "        \n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name,param,self.epoch_count)\n",
    "            self.tb.add_histogram(f'{name}.grad',param.grad,self.epoch_count)\n",
    "            \n",
    "        results = OrderedDict()\n",
    "        results[\"run\"]=self.run_count\n",
    "        results[\"epoch\"]=self.epoch_count\n",
    "        results[\"loss\"]=loss\n",
    "        results[\"accuracy\"]=accuracy\n",
    "        results[\"epoch duration\"]=epoch_duration\n",
    "        results[\"run duration\"]=run_duration\n",
    "                            \n",
    "        for k,v in self.run_params._asdict().items():  #deze komen uit run, je batch_size & lr\n",
    "            results[k] = v #geloof dat je hier dus voor elke run met andere batch size etc. maar 1 lr en batchsize toevoegt, vandaar dat dit niet in de loop zit\n",
    "        self.run_data.append(results) #1 batch_size en lr bij de results bij, en vervolgens voeg je al je results toe aan wat je metadata i guess\n",
    "        df = pd.DataFrame.from_dict(self.run_data,orient='columns') #dit zorgt dat het in een leuk tabelletje staat\n",
    "                                \n",
    "        clear_output(wait=True)\n",
    "        display(df)\n",
    "                                \n",
    "    def track_loss(self,loss):\n",
    "            self.epoch_loss += loss.item()* self.loader.batch_size\n",
    "                                    \n",
    "    def track_num_correct(self,preds,labels):\n",
    "            self.epoch_num_correct += self._get_num_correct(preds,labels)\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def _get_num_correct(self,preds,labels):\n",
    "        return preds.argmax(dim=1).eq(labels).sum().item()\n",
    "    \n",
    "    def save(self,fileName,ResDir,file_num):\n",
    "        \n",
    "        os.mkdir(f'{ResDir}\\{file_num}')\n",
    "        \n",
    "        pd.DataFrame.from_dict(\n",
    "        self.run_data\n",
    "        ,orient = 'columns'\n",
    "        ).to_csv(f'{ResDir}\\{file_num}\\{fileName}.csv')\n",
    "        \n",
    "        with open(f'{ResDir}\\{file_num}\\{fileName}.json','w',encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "#Network\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t,dim=1)\n",
    "\n",
    "        return t\n",
    "\n",
    "#computations\n",
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()\n",
    "                                                                                         ])\n",
    ")\n",
    "\n",
    "params = OrderedDict(\n",
    "    lr = [.001]\n",
    "    ,batch_size = [10000]\n",
    ")\n",
    "ResDir = 'runs_results'\n",
    "file_num = '0608_1653'\n",
    "m=RunManager()\n",
    "for run in RunBuilder.get_runs(params): #[Run(lr=..,batch_size=..),Run(lr=..,batch_size=..)]\n",
    "    \n",
    "    network = Network() #create network\n",
    "    loader = DataLoader(train_set,batch_size=run.batch_size) #load this run's data according to batch_size and lr\n",
    "    optimiser = optim.Adam(network.parameters(),lr=run.lr)\n",
    "    \n",
    "    m.begin_run(run,network,loader) #start met tellen van een aantal zaken\n",
    "    for epoch in range(1):\n",
    "        m.begin_epoch() #start met tellen voor epoch specifieke zaken\n",
    "        for batch in loader:\n",
    "            \n",
    "            images,labels=batch #laad specifieke batch\n",
    "            preds = network(images) #pass batch\n",
    "            loss = F.cross_entropy(preds,labels) # calculate loss\n",
    "            optimiser.zero_grad() # Zero gradients\n",
    "            loss.backward() # calculate gradients\n",
    "            optimiser.step() # update weights\n",
    "            \n",
    "            m.track_loss(loss) #epoch loss is updated. each epoch at beginning it is reset to 0.\n",
    "            m.track_num_correct(preds,labels) #same for num_correct\n",
    "        m.end_epoch() #each epoch's accuracy and loss are written to tb. no num_correct, it's there in accuracy\n",
    "    m.end_run()\n",
    "m.save(NameFile.nameit(params),ResDir,file_num) #mooie naam voor de results file met alle parameter values erin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
