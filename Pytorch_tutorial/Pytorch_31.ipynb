{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.1\n",
      "0.6.1\n",
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "torch.set_printoptions(linewidth=120)\n",
    "torch.set_grad_enabled(True)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_correct(preds, labels):\n",
    "    return preds.argmax(dim=1).eq(labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=12, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=12*4*4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=60)\n",
    "        self.out = nn.Linear(in_features=60, out_features=10)\n",
    "\n",
    "    def forward(self, t):\n",
    "        # (1) input layer\n",
    "        t = t\n",
    "\n",
    "        # (2) hidden conv layer\n",
    "        t = self.conv1(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (3) hidden conv layer\n",
    "        t = self.conv2(t)\n",
    "        t = F.relu(t)\n",
    "        t = F.max_pool2d(t, kernel_size=2, stride=2)\n",
    "\n",
    "        # (4) hidden linear layer\n",
    "        t = t.reshape(-1, 12*4*4)\n",
    "        t = self.fc1(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (5) hidden linear layer\n",
    "        t = self.fc2(t)\n",
    "        t = F.relu(t)\n",
    "\n",
    "        # (6) output layer\n",
    "        t = self.out(t)\n",
    "        #t = F.softmax(t,dim=1)\n",
    "\n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.FashionMNIST(\n",
    "    root='./data/FashionMNIST', train=True, download=True, transform=transforms.Compose([transforms.ToTensor()\n",
    "                                                                                         ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 total_correct: 47510 loss: 329.6513590812683\n",
      "epoch 1 total_correct: 51515 loss: 227.80737529695034\n",
      "epoch 2 total_correct: 52111 loss: 210.97554579377174\n",
      "epoch 3 total_correct: 52678 loss: 198.9040331840515\n",
      "epoch 4 total_correct: 52815 loss: 194.06669433414936\n",
      "epoch 5 total_correct: 52958 loss: 190.5855732858181\n",
      "epoch 6 total_correct: 52999 loss: 188.99202919006348\n",
      "epoch 7 total_correct: 53246 loss: 182.23907610028982\n",
      "epoch 8 total_correct: 53294 loss: 181.2956200018525\n",
      "epoch 9 total_correct: 53278 loss: 181.30406533926725\n"
     ]
    }
   ],
   "source": [
    "network = Network()\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=100,shuffle=True)\n",
    "optimiser = optim.Adam(network.parameters(),lr=0.01)\n",
    "\n",
    "images,labels = next(iter(train_loader))\n",
    "grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "tb = SummaryWriter()\n",
    "tb.add_image('images',grid)\n",
    "tb.add_graph(network,images)\n",
    "\n",
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss=0\n",
    "    total_correct=0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        images, labels = batch\n",
    "        \n",
    "        preds = network(images)\n",
    "        loss = F.cross_entropy(preds,labels)\n",
    "        \n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        total_loss+=loss.item()\n",
    "        total_correct+=get_num_correct(preds,labels)\n",
    "        \n",
    "\n",
    "    tb.add_scalar('Loss',total_loss,epoch)\n",
    "    tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "    tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
    "\n",
    "    for name, weight in network.named_parameters():\n",
    "        tb.add_histogram(name,weight,epoch)\n",
    "        tb.add_histogram(f'{name}.grad',weight.grad,epoch)\n",
    "        \n",
    "\n",
    "    print(\"epoch\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss )\n",
    "\n",
    "tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight torch.Size([6, 1, 5, 5])\n",
      "conv1.bias torch.Size([6])\n",
      "conv2.weight torch.Size([12, 6, 5, 5])\n",
      "conv2.bias torch.Size([12])\n",
      "fc1.weight torch.Size([120, 192])\n",
      "fc1.bias torch.Size([120])\n",
      "fc2.weight torch.Size([60, 120])\n",
      "fc2.bias torch.Size([60])\n",
      "out.weight torch.Size([10, 60])\n",
      "out.bias torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    " for name, weight in network.named_parameters():\n",
    "    print(name,weight.shape)     #grad kan niet direct achter name, omdat name hier de string is, niet meer de parameter. bij weight nog wel, dus gaat daar goed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.weight.grad torch.Size([6, 1, 5, 5])\n",
      "conv1.bias.grad torch.Size([6])\n",
      "conv2.weight.grad torch.Size([12, 6, 5, 5])\n",
      "conv2.bias.grad torch.Size([12])\n",
      "fc1.weight.grad torch.Size([120, 192])\n",
      "fc1.bias.grad torch.Size([120])\n",
      "fc2.weight.grad torch.Size([60, 120])\n",
      "fc2.bias.grad torch.Size([60])\n",
      "out.weight.grad torch.Size([10, 60])\n",
      "out.bias.grad torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for name, weight in network.named_parameters():\n",
    "    print(f'{name}.grad',weight.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size 100 lr 0.01 epoch 0 total_correct: 47030 loss: 33979.10651564598\n",
      "batch size 100 lr 0.01 epoch 1 total_correct: 51170 loss: 23542.933164536953\n",
      "batch size 100 lr 0.001 epoch 0 total_correct: 41441 loss: 49015.39563238621\n",
      "batch size 100 lr 0.001 epoch 1 total_correct: 47837 loss: 32082.994809746742\n",
      "batch size 100 lr 0.0001 epoch 0 total_correct: 30387 loss: 84251.64348483086\n",
      "batch size 100 lr 0.0001 epoch 1 total_correct: 42173 loss: 47428.92200946808\n",
      "batch size 100 lr 1e-05 epoch 0 total_correct: 6381 loss: 137812.89863586426\n",
      "batch size 100 lr 1e-05 epoch 1 total_correct: 19705 loss: 131932.2633743286\n",
      "batch size 1000 lr 0.01 epoch 0 total_correct: 34338 loss: 66970.43001651764\n",
      "batch size 1000 lr 0.01 epoch 1 total_correct: 46293 loss: 35105.012238025665\n",
      "batch size 1000 lr 0.001 epoch 0 total_correct: 26925 loss: 94913.51974010468\n",
      "batch size 1000 lr 0.001 epoch 1 total_correct: 41615 loss: 48108.62183570862\n",
      "batch size 1000 lr 0.0001 epoch 0 total_correct: 9892 loss: 137229.31146621704\n",
      "batch size 1000 lr 0.0001 epoch 1 total_correct: 20486 loss: 128130.59341907501\n",
      "batch size 1000 lr 1e-05 epoch 0 total_correct: 6001 loss: 138244.5456981659\n",
      "batch size 1000 lr 1e-05 epoch 1 total_correct: 6261 loss: 138117.22922325134\n",
      "batch size 10000 lr 0.01 epoch 0 total_correct: 11446 loss: 128947.61204719543\n",
      "batch size 10000 lr 0.01 epoch 1 total_correct: 25620 loss: 86888.12255859375\n",
      "batch size 10000 lr 0.001 epoch 0 total_correct: 6667 loss: 137348.6566543579\n",
      "batch size 10000 lr 0.001 epoch 1 total_correct: 17562 loss: 133435.35900115967\n",
      "batch size 10000 lr 0.0001 epoch 0 total_correct: 5996 loss: 138207.51905441284\n",
      "batch size 10000 lr 0.0001 epoch 1 total_correct: 6004 loss: 138140.11096954346\n",
      "batch size 10000 lr 1e-05 epoch 0 total_correct: 5773 loss: 138305.15146255493\n",
      "batch size 10000 lr 1e-05 epoch 1 total_correct: 5779 loss: 138293.65253448486\n"
     ]
    }
   ],
   "source": [
    "#we resetten gewoon vrolijk\n",
    "batch_size_list = [100, 1000, 10000] #we zien dat hoe groter batch size, hoe slechter de guess\n",
    "lr_list = [0.01, .001, .0001, .00001] #we zien dat ook kleinste lr 't beste werkt. we skippen nog niet 'genoeg iig' de optima\n",
    "\n",
    "for batch_size in batch_size_list:#dit werkt dus zonder te initieren, hij begrijpt zelf wat ie moet zijn. zelfde voor lr\n",
    "    for lr in lr_list:\n",
    "        network = Network() #reset dus elke keer ook de weights op deze manier denk ik\n",
    "        train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n",
    "        optimiser = optim.Adam(network.parameters(),lr=lr)\n",
    "\n",
    "        images,labels = next(iter(train_loader))\n",
    "        grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "        comment = f' batch_size={batch_size} lr ={lr}'#geloof dat je dit alles kunt noemen, parameters zelfde naam geven is super stupide, les 1\n",
    "        tb = SummaryWriter(comment=comment)\n",
    "        tb.add_image('images',grid)\n",
    "        tb.add_graph(network,images)\n",
    "        \n",
    "        for epoch in range(2):\n",
    "    \n",
    "            total_loss=0\n",
    "            total_loss_b=0\n",
    "            total_correct=0\n",
    "    \n",
    "            for batch in train_loader:\n",
    "                images, labels = batch\n",
    "        \n",
    "                preds = network(images)\n",
    "                loss = F.cross_entropy(preds,labels)\n",
    "            \n",
    "                optimiser.zero_grad()\n",
    "                loss.backward() #zoals je ziet wordt er al in epoch 1 geoptimised. dus de initial guess wordt wel zeker aangetast door de learning rate\n",
    "                optimiser.step()\n",
    "        \n",
    "                total_loss+=loss.item()*batch_size #loss is average. to be able to compare the total if we worked with different batch sizes in different, we have to make average -> total\n",
    "                total_correct+=get_num_correct(preds,labels)\n",
    "        \n",
    "\n",
    "            tb.add_scalar('Loss',total_loss,epoch)\n",
    "            tb.add_scalar('Number Correct',total_correct,epoch)\n",
    "            tb.add_scalar('Accuracy',total_correct/len(train_set),epoch)\n",
    "\n",
    "            for name, weight in network.named_parameters():\n",
    "                tb.add_histogram(name,weight,epoch)\n",
    "                tb.add_histogram(f'{name}.grad',weight.grad,epoch)\n",
    "        \n",
    "\n",
    "            print(\"batch size\",batch_size,\"lr\",lr,\"epoch\",epoch,\"total_correct:\",total_correct,\"loss:\",total_loss )\n",
    "    \n",
    "    tb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = dict(\n",
    "    lr= [0.01, 0.001]\n",
    "    ,batch_size = [10, 100, 1000]\n",
    "    ,shuffel = [True, False]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.01, 0.001], [10, 100, 1000], [True, False]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_values = [v for v in parameters.values()]   #return value list v for each v in the parameter dictionary. basically sort of creating a tensor where concatenating different v's after one another? \n",
    "param_values #so lr/batch_size/shuffle are al v's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': [0.01, 0.001], 'batch_size': [10, 100, 1000], 'shuffel': [True, False]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param = [v in parameters.values()] #dit werkt dus niet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 10 True\n",
      "0.01 10 False\n",
      "0.01 100 True\n",
      "0.01 100 False\n",
      "0.01 1000 True\n",
      "0.01 1000 False\n",
      "0.001 10 True\n",
      "0.001 10 False\n",
      "0.001 100 True\n",
      "0.001 100 False\n",
      "0.001 1000 True\n",
      "0.001 1000 False\n"
     ]
    }
   ],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values): #star here tells product to treat each value in the list as argument, instead of treating the list itself as the argument\n",
    "    print(lr,batch_size,shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01, 0.001]\n",
      "[10, 100, 1000]\n",
      "[True, False]\n"
     ]
    }
   ],
   "source": [
    "for lr in (param_values):\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-16-21bedce64674>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-21bedce64674>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    for lr, batch_size, shuffle in product(*param_values): #star here tells product to treat each value in the list as argument, instead of treating the list itself as the argument\u001b[0m\n\u001b[1;37m                                                                                                                                                                                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "for lr, batch_size, shuffle in product(*param_values): #star here tells product to treat each value in the list as argument, instead of treating the list itself as the argument\n",
    "    comment_smart = f'lr{lr},batch_size{batch_size},shuffle {shuffle}''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
